<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Python Data Analyst Interview Prep</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
      line-height: 1.6;
      color: #fff;
      background: linear-gradient(135deg, #1a0033 0%, #220044 50%, #1a0033 100%);
      min-height: 100vh;
      position: relative;
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2rem;
    }

    .header {
      text-align: center;
      margin-bottom: 3rem;
      animation: fadeInUp 0.8s ease-out;
    }

    h1 {
      font-size: 3rem;
      font-weight: 700;
      background: linear-gradient(135deg, #fff 0%, #e1e5f2 50%, #fff 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      text-shadow: 0 0 30px rgba(255, 255, 255, 0.3);
      margin-bottom: 1rem;
    }

    .subtitle {
      font-size: 1.2rem;
      color: rgba(255, 255, 255, 0.8);
      margin-bottom: 2rem;
    }

    .mode-switcher {
      display: flex;
      justify-content: center;
      gap: 1rem;
      margin-bottom: 3rem;
    }

    .mode-btn {
      padding: 1rem 2rem;
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.25) 0%, rgba(255, 255, 255, 0.1) 100%);
      backdrop-filter: blur(15px);
      border: 2px solid rgba(255, 255, 255, 0.3);
      border-radius: 50px;
      color: white;
      font-size: 1.1rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.15);
    }

    .mode-btn:hover {
      transform: translateY(-2px);
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.35) 0%, rgba(255, 255, 255, 0.15) 100%);
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
    }

    .mode-btn.active {
      background: linear-gradient(135deg, rgba(100, 200, 255, 0.4) 0%, rgba(100, 200, 255, 0.2) 100%);
      border-color: rgba(100, 200, 255, 0.6);
      box-shadow: 0 0 20px rgba(100, 200, 255, 0.3);
    }

    /* Test Selector */
    .test-selector {
      text-align: center;
      margin-bottom: 3rem;
      animation: fadeInUp 0.8s ease-out;
    }

    .test-selector-title {
      font-size: 1.5rem;
      color: rgba(255, 255, 255, 0.9);
      margin-bottom: 1.5rem;
    }

    .test-options-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 1.5rem;
      max-width: 900px;
      margin: 0 auto;
    }

    .test-option-card {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.15) 0%, rgba(255, 255, 255, 0.05) 100%);
      backdrop-filter: blur(20px);
      border: 2px solid rgba(255, 255, 255, 0.2);
      border-radius: 20px;
      padding: 2rem;
      cursor: pointer;
      transition: all 0.3s ease;
      text-align: center;
    }

    .test-option-card:hover {
      transform: translateY(-5px);
      background: linear-gradient(135deg, rgba(100, 200, 255, 0.25) 0%, rgba(100, 200, 255, 0.1) 100%);
      border-color: rgba(100, 200, 255, 0.4);
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
    }

    .test-option-icon {
      font-size: 2.5rem;
      margin-bottom: 1rem;
    }

    .test-option-title {
      font-size: 1.2rem;
      font-weight: 600;
      color: white;
      margin-bottom: 0.5rem;
    }

    .test-option-count {
      font-size: 0.9rem;
      color: rgba(255, 255, 255, 0.7);
    }

    /* Study Mode Styles */
    .study-content {
      display: none;
    }

    .study-content.active {
      display: block;
    }

    /* Topic Navigation */
    .topic-nav {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.1) 0%, rgba(255, 255, 255, 0.05) 100%);
      backdrop-filter: blur(20px);
      border-radius: 25px;
      padding: 2rem;
      margin-bottom: 3rem;
      border: 1px solid rgba(255, 255, 255, 0.2);
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
    }

    .topic-nav-title {
      font-size: 1.5rem;
      font-weight: 600;
      color: rgba(255, 255, 255, 0.95);
      text-align: center;
      margin-bottom: 1.5rem;
      text-shadow: 0 0 20px rgba(255, 255, 255, 0.3);
    }

    .topic-nav-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 1rem;
    }

    .topic-nav-btn {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.15) 0%, rgba(255, 255, 255, 0.05) 100%);
      backdrop-filter: blur(15px);
      border: 2px solid rgba(255, 255, 255, 0.2);
      border-radius: 20px;
      padding: 1rem;
      color: white;
      font-size: 0.95rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      text-align: center;
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 0.5rem;
    }

    .topic-nav-btn:hover {
      background: linear-gradient(135deg, rgba(100, 200, 255, 0.25) 0%, rgba(100, 200, 255, 0.1) 100%);
      border-color: rgba(100, 200, 255, 0.4);
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.2);
    }

    .topic-nav-icon {
      font-size: 1.2rem;
    }

    .category-section {
      margin-bottom: 3rem;
    }

    .category-title {
      font-size: 2rem;
      font-weight: 600;
      color: rgba(255, 255, 255, 0.95);
      margin-bottom: 2rem;
      text-align: center;
      text-shadow: 0 0 20px rgba(255, 255, 255, 0.3);
    }

    .question-card {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.15) 0%, rgba(255, 255, 255, 0.05) 100%);
      backdrop-filter: blur(20px);
      border-radius: 20px;
      padding: 2rem;
      margin-bottom: 2rem;
      border: 1px solid rgba(255, 255, 255, 0.2);
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
      transition: all 0.3s ease;
    }

    .question-card:hover {
      transform: translateY(-3px);
      box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);
    }

    .question {
      font-size: 1.3rem;
      font-weight: 600;
      color: rgba(255, 255, 255, 0.95);
      margin-bottom: 1rem;
    }

    .answer {
      color: rgba(255, 255, 255, 0.8);
      margin-bottom: 1rem;
      line-height: 1.8;
    }

    .code-block {
      background: rgba(0, 0, 0, 0.4);
      border: 1px solid rgba(255, 255, 255, 0.1);
      border-radius: 10px;
      padding: 1rem;
      margin: 1rem 0;
      overflow-x: auto;
      font-family: 'Consolas', 'Monaco', monospace;
      font-size: 0.9rem;
      color: #e8e8e8;
    }

    .code-block pre {
      margin: 0;
      white-space: pre-wrap;
      word-wrap: break-word;
    }

    /* Test Mode Styles */
    .test-content {
      display: none;
    }

    .test-content.active {
      display: block;
    }

    .test-header {
      text-align: center;
      margin-bottom: 2rem;
    }

    .test-title {
      font-size: 1.8rem;
      color: rgba(255, 255, 255, 0.9);
      margin-bottom: 0.5rem;
    }

    .test-subtitle {
      font-size: 1rem;
      color: rgba(255, 255, 255, 0.7);
    }

    .test-progress {
      background: rgba(255, 255, 255, 0.1);
      border-radius: 10px;
      height: 10px;
      margin-bottom: 2rem;
      overflow: hidden;
    }

    .test-progress-bar {
      background: linear-gradient(90deg, #4CAF50 0%, #8BC34A 100%);
      height: 100%;
      width: 0%;
      transition: width 0.3s ease;
    }

    .test-question-card {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.2) 0%, rgba(255, 255, 255, 0.08) 100%);
      backdrop-filter: blur(20px);
      border-radius: 25px;
      padding: 3rem;
      margin-bottom: 2rem;
      border: 1px solid rgba(255, 255, 255, 0.25);
      box-shadow: 0 10px 40px rgba(0, 0, 0, 0.25);
    }

    .test-question {
      font-size: 1.5rem;
      font-weight: 600;
      color: white;
      margin-bottom: 2rem;
      text-align: center;
    }

    .test-options {
      display: grid;
      gap: 1rem;
    }

    .test-option {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.15) 0%, rgba(255, 255, 255, 0.05) 100%);
      border: 2px solid rgba(255, 255, 255, 0.2);
      border-radius: 15px;
      padding: 1.5rem;
      cursor: pointer;
      transition: all 0.3s ease;
      color: rgba(255, 255, 255, 0.9);
      font-size: 1.1rem;
      line-height: 1.6;
    }

    .test-option:hover {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.25) 0%, rgba(255, 255, 255, 0.1) 100%);
      border-color: rgba(255, 255, 255, 0.4);
      transform: translateY(-2px);
      box-shadow: 0 8px 20px rgba(0, 0, 0, 0.2);
    }

    .test-option.correct {
      background: linear-gradient(135deg, rgba(76, 175, 80, 0.3) 0%, rgba(76, 175, 80, 0.1) 100%);
      border-color: rgba(76, 175, 80, 0.6);
    }

    .test-option.incorrect {
      background: linear-gradient(135deg, rgba(244, 67, 54, 0.3) 0%, rgba(244, 67, 54, 0.1) 100%);
      border-color: rgba(244, 67, 54, 0.6);
    }

    .test-option.disabled {
      pointer-events: none;
      opacity: 0.7;
    }

    .test-navigation {
      display: flex;
      justify-content: center;
      gap: 1rem;
      margin-top: 2rem;
    }

    .nav-btn {
      padding: 0.8rem 2rem;
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.25) 0%, rgba(255, 255, 255, 0.1) 100%);
      border: 2px solid rgba(255, 255, 255, 0.3);
      border-radius: 30px;
      color: white;
      font-size: 1rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
    }

    .nav-btn:hover:not(:disabled) {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.35) 0%, rgba(255, 255, 255, 0.15) 100%);
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.2);
    }

    .nav-btn:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    .test-results {
      display: none;
      text-align: center;
      padding: 3rem;
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.2) 0%, rgba(255, 255, 255, 0.08) 100%);
      backdrop-filter: blur(20px);
      border-radius: 25px;
      border: 1px solid rgba(255, 255, 255, 0.25);
      box-shadow: 0 10px 40px rgba(0, 0, 0, 0.25);
    }

    .test-results.active {
      display: block;
    }

    .results-title {
      font-size: 2.5rem;
      font-weight: 700;
      color: white;
      margin-bottom: 1rem;
    }

    .results-score {
      font-size: 4rem;
      font-weight: 700;
      background: linear-gradient(135deg, #4CAF50 0%, #8BC34A 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      margin-bottom: 2rem;
    }

    .results-message {
      font-size: 1.3rem;
      color: rgba(255, 255, 255, 0.8);
      margin-bottom: 2rem;
    }

    .restart-btn {
      padding: 1rem 3rem;
      background: linear-gradient(135deg, rgba(100, 200, 255, 0.4) 0%, rgba(100, 200, 255, 0.2) 100%);
      border: 2px solid rgba(100, 200, 255, 0.6);
      border-radius: 50px;
      color: white;
      font-size: 1.2rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      margin: 0.5rem;
    }

    .restart-btn:hover {
      background: linear-gradient(135deg, rgba(100, 200, 255, 0.5) 0%, rgba(100, 200, 255, 0.3) 100%);
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(100, 200, 255, 0.3);
    }

    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .back-btn {
      position: fixed;
      top: 2rem;
      left: 2rem;
      padding: 0.8rem 1.5rem;
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.25) 0%, rgba(255, 255, 255, 0.1) 100%);
      backdrop-filter: blur(15px);
      border: 2px solid rgba(255, 255, 255, 0.3);
      border-radius: 30px;
      color: white;
      text-decoration: none;
      font-weight: 600;
      transition: all 0.3s ease;
      z-index: 100;
    }

    .back-btn:hover {
      background: linear-gradient(135deg, rgba(255, 255, 255, 0.35) 0%, rgba(255, 255, 255, 0.15) 100%);
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.2);
    }
  </style>
</head>
<body>
  <a href="../" class="back-btn">‚Üê Back to Portfolio</a>

  <div class="container">
    <header class="header">
      <h1>Python Data Analyst Interview Prep</h1>
      <p class="subtitle">Master key concepts with interactive study and test modes</p>
    </header>

    <div class="mode-switcher">
      <button class="mode-btn active" onclick="switchMode('study')">üìö Study Mode</button>
      <button class="mode-btn" onclick="switchMode('test')">üéØ Test Mode</button>
    </div>

    <!-- Study Mode Content -->
    <div class="study-content active" id="studyContent">
      <!-- Topic Navigation will be dynamically generated -->
      <div class="topic-nav" id="topicNav">
        <h2 class="topic-nav-title">üìö Quick Navigation</h2>
        <div class="topic-nav-grid" id="topicNavGrid">
          <!-- Navigation buttons will be dynamically generated -->
        </div>
      </div>
      
      <!-- Question content will be dynamically generated -->
      <div id="studyQuestions">
        <!-- Content will be dynamically generated -->
      </div>
    </div>

    <!-- Test Selector -->
    <div class="test-selector" id="testSelector" style="display: none;">
      <h2 class="test-selector-title">Choose Your Test</h2>
      <div class="test-options-grid" id="testOptionsGrid">
        <!-- Test options will be dynamically generated -->
      </div>
    </div>

    <!-- Test Mode Content -->
    <div class="test-content" id="testContent">
      <div class="test-header">
        <h2 class="test-title" id="testTitle">All Topics Test</h2>
        <p class="test-subtitle" id="testSubtitle">20 questions from all categories</p>
      </div>
      
      <div class="test-progress">
        <div class="test-progress-bar" id="progressBar"></div>
      </div>
      
      <div class="test-question-card" id="testQuestionCard">
        <!-- Question will be dynamically generated -->
      </div>

      <div class="test-navigation">
        <button class="nav-btn" id="prevBtn" onclick="previousQuestion()" disabled>Previous</button>
        <button class="nav-btn" id="nextBtn" onclick="nextQuestion()">Next</button>
      </div>
    </div>

    <!-- Test Results -->
    <div class="test-results" id="testResults">
      <h2 class="results-title">Test Complete!</h2>
      <div class="results-score" id="resultsScore"></div>
      <p class="results-message" id="resultsMessage"></p>
      <button class="restart-btn" onclick="retakeCurrentTest()">Retake This Test</button>
      <button class="restart-btn" onclick="switchMode('test')">Choose Another Test</button>
    </div>
  </div>

  <script>
    // Question data structure
    const questionData = {
      "Python Fundamentals": [
        {
          question: "What's the difference between a list and a tuple?",
          answer: "Lists are mutable (can be changed), tuples are immutable. Lists use square brackets [], tuples use parentheses (). Lists have methods like append(), while tuples don't.",
          code: `# List - mutable
my_list = [1, 2, 3]
my_list.append(4)  # Works

# Tuple - immutable
my_tuple = (1, 2, 3)
# my_tuple.append(4)  # Would raise AttributeError`,
          wrongAnswers: [
            "Lists are immutable, tuples are mutable. Lists use parentheses, tuples use square brackets.",
            "There is no difference, they are interchangeable data structures.",
            "Lists can only store strings, tuples can store any data type."
          ]
        },
        {
          question: "Explain list comprehensions vs regular loops",
          answer: "List comprehensions are more concise and often faster. They create lists in a single line.",
          code: `# Regular loop
squares = []
for i in range(10):
    squares.append(i**2)

# List comprehension
squares = [i**2 for i in range(10)]

# With condition
even_squares = [i**2 for i in range(10) if i % 2 == 0]`,
          wrongAnswers: [
            "List comprehensions are slower but more readable than regular loops.",
            "Regular loops create lists in a single line, list comprehensions require multiple lines.",
            "List comprehensions can only be used with strings, not numbers."
          ]
        },
        {
          question: "What are *args and **kwargs?",
          answer: "*args passes variable number of positional arguments, **kwargs passes variable number of keyword arguments.",
          code: `def my_function(*args, **kwargs):
    print("Args:", args)
    print("Kwargs:", kwargs)

my_function(1, 2, 3, name="John", age=30)
# Args: (1, 2, 3)
# Kwargs: {'name': 'John', 'age': 30}`,
          wrongAnswers: [
            "*args is for keyword arguments, **kwargs is for positional arguments.",
            "Both *args and **kwargs are used for the same purpose - passing lists to functions.",
            "*args can only accept strings, **kwargs can only accept numbers."
          ]
        },
        {
          question: "What's the difference between shallow copy and deep copy?",
          answer: "Shallow copy creates a new object but references to nested objects remain the same. Deep copy creates completely independent copies of all nested objects.",
          code: `import copy

# Original list with nested list
original = [[1, 2], [3, 4]]

# Shallow copy
shallow = original.copy()
shallow[0][0] = 999  # Modifies original too!

# Deep copy
deep = copy.deepcopy(original)
deep[0][0] = 999  # Original remains unchanged`,
          wrongAnswers: [
            "Shallow copy and deep copy are the same operation with different names.",
            "Deep copy only copies the first level, shallow copy copies all levels.",
            "Shallow copy is always faster and should be used in all cases."
          ]
        },
        {
          question: "Explain Python's Global Interpreter Lock (GIL) and its impact on multi-threading",
          answer: "The GIL is a mutex that allows only one thread to execute Python bytecode at a time. This makes multi-threading ineffective for CPU-bound tasks but still useful for I/O-bound tasks.",
          code: `# CPU-bound task - GIL limits performance
import threading
import time

def cpu_bound():
    count = 0
    for i in range(100_000_000):
        count += 1

# Multi-threading won't speed this up due to GIL
# Use multiprocessing for CPU-bound tasks instead

# I/O-bound task - threading still helps
def io_bound():
    time.sleep(1)  # Simulates I/O operation
    
# Threading is effective here because GIL is released during I/O`,
          wrongAnswers: [
            "The GIL makes Python completely single-threaded and prevents any parallel execution.",
            "The GIL improves performance by preventing race conditions in all cases.",
            "The GIL only affects multiprocessing, not multithreading."
          ]
        }
      ],
      "Pandas Questions": [
        {
          question: "How do you handle missing values in pandas?",
          answer: "Multiple approaches: Check with isnull()/info(), drop with dropna(), or fill with fillna() using various strategies.",
          code: `import pandas as pd
import numpy as np

df = pd.DataFrame({'A': [1, 2, np.nan], 'B': [4, np.nan, 6]})

# Check for missing values
df.isnull().sum()
df.info()

# Drop missing values
df.dropna()  # Drop rows with any NaN
df.dropna(axis=1)  # Drop columns with any NaN
df.dropna(thresh=2)  # Keep rows with at least 2 non-null values

# Fill missing values
df.fillna(0)  # Fill with 0
df.fillna(df.mean())  # Fill with mean
df.fillna(method='ffill')  # Forward fill
df.fillna(method='bfill')  # Backward fill`,
          wrongAnswers: [
            "Pandas automatically handles missing values, no action needed.",
            "The only way to handle missing values is to delete the entire dataset.",
            "Missing values can only be filled with the value -999."
          ]
        },
        {
          question: "Explain groupby operations",
          answer: "GroupBy splits data into groups based on criteria, applies functions, and combines results.",
          code: `df = pd.DataFrame({
    'Category': ['A', 'B', 'A', 'B', 'A'],
    'Values': [10, 20, 30, 40, 50],
    'Count': [1, 2, 3, 4, 5]
})

# Basic groupby
df.groupby('Category').sum()
df.groupby('Category')['Values'].mean()

# Multiple aggregations
df.groupby('Category').agg({
    'Values': ['sum', 'mean'],
    'Count': 'max'
})

# Custom aggregation
df.groupby('Category').apply(lambda x: x['Values'].max() - x['Values'].min())`,
          wrongAnswers: [
            "GroupBy only works with numerical data, not categorical data.",
            "GroupBy permanently modifies the original DataFrame structure.",
            "GroupBy can only perform one operation at a time on the data."
          ]
        },
        {
          question: "How do you merge/join DataFrames?",
          answer: "Use merge(), join(), or concat() depending on the situation.",
          code: `df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})
df2 = pd.DataFrame({'key': ['A', 'B', 'D'], 'value2': [4, 5, 6]})

# Inner join (default)
pd.merge(df1, df2, on='key')

# Left join
pd.merge(df1, df2, on='key', how='left')

# Outer join
pd.merge(df1, df2, on='key', how='outer')

# Concatenate
pd.concat([df1, df2], axis=0)  # Vertically
pd.concat([df1, df2], axis=1)  # Horizontally`,
          wrongAnswers: [
            "DataFrames can only be merged if they have exactly the same columns.",
            "merge() and join() are the same function with different names.",
            "concat() can only be used to join DataFrames vertically, not horizontally."
          ]
        },
        {
          question: "How would you handle duplicate rows in a DataFrame?",
          answer: "Use duplicated() to identify duplicates and drop_duplicates() to remove them. You can specify subset of columns and keep first/last occurrence.",
          code: `# Create sample data with duplicates
df = pd.DataFrame({
    'A': [1, 1, 2, 2, 3],
    'B': ['x', 'x', 'y', 'y', 'z'],
    'C': [10, 10, 20, 30, 40]
})

# Check for duplicates
df.duplicated()  # Returns boolean Series
df.duplicated(subset=['A', 'B'])  # Check specific columns

# Remove duplicates
df.drop_duplicates()  # Remove all duplicates
df.drop_duplicates(subset=['A', 'B'])  # Based on specific columns
df.drop_duplicates(keep='last')  # Keep last occurrence
df.drop_duplicates(keep=False)  # Remove all occurrences`,
          wrongAnswers: [
            "Pandas cannot detect duplicate rows, you must manually check each row.",
            "drop_duplicates() permanently modifies the original DataFrame without option to create a copy.",
            "Duplicates can only be removed based on all columns, not a subset."
          ]
        },
        {
          question: "Explain the difference between loc and iloc in pandas",
          answer: "loc is label-based indexing (uses index/column names), iloc is integer position-based indexing (uses integer positions).",
          code: `df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': [5, 6, 7, 8],
    'C': [9, 10, 11, 12]
}, index=['row1', 'row2', 'row3', 'row4'])

# loc - label based
df.loc['row1']  # Select by index label
df.loc['row1', 'A']  # Specific cell
df.loc['row1':'row3', 'A':'B']  # Slice (inclusive)
df.loc[df['A'] > 2]  # Boolean indexing

# iloc - integer position based
df.iloc[0]  # First row
df.iloc[0, 0]  # First cell
df.iloc[0:3, 0:2]  # Slice (exclusive end)
df.iloc[[0, 2], [0, 2]]  # Specific positions`,
          wrongAnswers: [
            "loc and iloc are identical and can be used interchangeably.",
            "iloc uses column names while loc uses integer positions.",
            "loc can only select rows, iloc can only select columns."
          ]
        }
      ],
      "NumPy Questions": [
        {
          question: "What's the difference between np.array and Python lists for numerical operations?",
          answer: "NumPy arrays are faster, use less memory, and support vectorized operations.",
          code: `import numpy as np

# Python lists
list1 = [1, 2, 3, 4, 5]
list2 = [6, 7, 8, 9, 10]
# list1 + list2  # Concatenates, doesn't add elementwise

# NumPy arrays
arr1 = np.array([1, 2, 3, 4, 5])
arr2 = np.array([6, 7, 8, 9, 10])
arr1 + arr2  # Element-wise addition
arr1 * 2     # Scalar multiplication
np.sqrt(arr1)  # Element-wise square root`,
          wrongAnswers: [
            "Python lists are faster than NumPy arrays for numerical operations.",
            "NumPy arrays and Python lists behave exactly the same for mathematical operations.",
            "NumPy arrays can only store integers, while lists can store any type."
          ]
        },
        {
          question: "Explain broadcasting in NumPy",
          answer: "Broadcasting allows operations between arrays of different shapes.",
          code: `# 1D array + scalar
arr = np.array([1, 2, 3])
arr + 10  # [11, 12, 13]

# 2D array + 1D array
matrix = np.array([[1, 2, 3], [4, 5, 6]])
vector = np.array([10, 20, 30])
matrix + vector  # Adds vector to each row`,
          wrongAnswers: [
            "Broadcasting only works with arrays of exactly the same shape.",
            "Broadcasting requires explicit reshaping of arrays before operations.",
            "Broadcasting is a feature that slows down NumPy operations."
          ]
        },
        {
          question: "How do you efficiently compute statistics along specific axes in NumPy?",
          answer: "Use axis parameter in NumPy functions. axis=0 operates along columns (down), axis=1 operates along rows (across).",
          code: `arr = np.array([[1, 2, 3],
                [4, 5, 6],
                [7, 8, 9]])

# Along columns (axis=0)
np.mean(arr, axis=0)  # [4., 5., 6.]
np.sum(arr, axis=0)   # [12, 15, 18]

# Along rows (axis=1)
np.mean(arr, axis=1)  # [2., 5., 8.]
np.sum(arr, axis=1)   # [6, 15, 24]

# Multiple statistics at once
np.min(arr, axis=0), np.max(arr, axis=0)
np.std(arr, axis=1), np.var(arr, axis=1)`,
          wrongAnswers: [
            "NumPy can only compute statistics on entire arrays, not along specific axes.",
            "axis=0 operates along rows, axis=1 operates along columns.",
            "You must use loops to compute statistics along specific dimensions."
          ]
        },
        {
          question: "What's the difference between np.dot(), np.matmul(), and the @ operator?",
          answer: "np.dot() handles both dot products and matrix multiplication with specific rules for higher dimensions. np.matmul() and @ are strict matrix multiplication operators that broadcast differently.",
          code: `a = np.array([[1, 2], [3, 4]])
b = np.array([[5, 6], [7, 8]])

# All three are equivalent for 2D arrays
np.dot(a, b)
np.matmul(a, b)
a @ b

# Differences appear with higher dimensions
c = np.random.rand(3, 4, 5)
d = np.random.rand(5, 6)

# matmul/@ broadcast over batch dimensions
np.matmul(c, d).shape  # (3, 4, 6)
(c @ d).shape          # (3, 4, 6)

# dot behaves differently
# np.dot(c, d) would sum over last axis of c and second-to-last of d`,
          wrongAnswers: [
            "All three operators are identical and interchangeable in all cases.",
            "@ operator can only be used for element-wise multiplication.",
            "np.dot() is deprecated and should never be used."
          ]
        },
        {
          question: "How do you handle memory efficiency with large NumPy arrays?",
          answer: "Use appropriate data types, views instead of copies, memory mapping for huge files, and in-place operations when possible.",
          code: `# 1. Choose appropriate data types
arr_float64 = np.array([1, 2, 3])  # Default: 8 bytes per element
arr_float32 = np.array([1, 2, 3], dtype=np.float32)  # 4 bytes
arr_int8 = np.array([1, 2, 3], dtype=np.int8)  # 1 byte

# 2. Use views instead of copies
arr = np.arange(1000000)
view = arr[::2]  # View, no copy
copy = arr[::2].copy()  # Explicit copy

# 3. Memory mapping for large files
mmap_arr = np.memmap('large_file.dat', dtype='float32', mode='r', shape=(1000000,))

# 4. In-place operations
arr += 10  # Modifies in place
arr = arr + 10  # Creates new array

# 5. Check memory usage
arr.nbytes  # Total bytes used`,
          wrongAnswers: [
            "NumPy automatically optimizes memory usage, no manual intervention needed.",
            "Using smaller data types always makes computations faster.",
            "Memory mapping is slower than loading entire arrays into RAM."
          ]
        }
      ],
      "SQL Questions": [
        {
          question: "Write a query to find the second highest salary",
          answer: "Multiple methods: Using LIMIT with OFFSET, subquery with MAX, or window functions like DENSE_RANK().",
          code: `-- Method 1: Using LIMIT and OFFSET
SELECT DISTINCT salary
FROM employees
ORDER BY salary DESC
LIMIT 1 OFFSET 1;

-- Method 2: Using subquery
SELECT MAX(salary)
FROM employees
WHERE salary < (SELECT MAX(salary) FROM employees);

-- Method 3: Using window functions
SELECT DISTINCT salary
FROM (
    SELECT salary,
           DENSE_RANK() OVER (ORDER BY salary DESC) as rank
    FROM employees
) ranked
WHERE rank = 2;`,
          wrongAnswers: [
            "SELECT salary FROM employees ORDER BY salary LIMIT 2",
            "SELECT TOP 2 salary FROM employees",
            "SELECT SECOND(salary) FROM employees"
          ]
        },
        {
          question: "Explain the difference between WHERE and HAVING",
          answer: "WHERE filters rows before grouping, HAVING filters groups after grouping.",
          code: `-- WHERE: filters individual rows
SELECT department, COUNT(*)
FROM employees
WHERE salary > 50000
GROUP BY department;

-- HAVING: filters groups
SELECT department, COUNT(*)
FROM employees
GROUP BY department
HAVING COUNT(*) > 5;`,
          wrongAnswers: [
            "WHERE and HAVING are interchangeable and can be used in any order.",
            "HAVING can only be used with the COUNT function.",
            "WHERE is used for strings, HAVING is used for numbers."
          ]
        },
        {
          question: "What are window functions and how do you use them for running totals and rankings?",
          answer: "Window functions perform calculations across a set of rows related to the current row. They're used with OVER clause for analytics like running totals, rankings, and moving averages.",
          code: `-- Running total
SELECT date, 
       amount,
       SUM(amount) OVER (ORDER BY date) as running_total
FROM sales;

-- Ranking
SELECT name,
       sales,
       RANK() OVER (ORDER BY sales DESC) as rank,
       DENSE_RANK() OVER (ORDER BY sales DESC) as dense_rank,
       ROW_NUMBER() OVER (ORDER BY sales DESC) as row_num
FROM salespeople;

-- Partitioned ranking (rank within each department)
SELECT department,
       name,
       salary,
       RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank
FROM employees;

-- Moving average
SELECT date,
       sales,
       AVG(sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7day
FROM daily_sales;`,
          wrongAnswers: [
            "Window functions can only be used with GROUP BY clauses.",
            "RANK() and ROW_NUMBER() always return the same results.",
            "Window functions cannot be used with PARTITION BY."
          ]
        },
        {
          question: "How do you handle NULL values in SQL comparisons and aggregations?",
          answer: "NULL requires special handling: use IS NULL/IS NOT NULL for comparisons, COALESCE/ISNULL for replacements. Most aggregates ignore NULL except COUNT(*).",
          code: `-- NULL comparisons
SELECT * FROM employees WHERE manager_id IS NULL;  -- Correct
-- WHERE manager_id = NULL  -- Wrong! Always returns no rows

-- COALESCE/ISNULL
SELECT name, COALESCE(phone, email, 'No contact') as contact
FROM customers;

-- Aggregations with NULL
-- COUNT(*) counts all rows including NULL
-- COUNT(column) ignores NULL values
SELECT COUNT(*) as total_rows,
       COUNT(bonus) as employees_with_bonus,
       AVG(bonus) as avg_bonus,  -- Ignores NULL
       AVG(COALESCE(bonus, 0)) as avg_bonus_treating_null_as_zero
FROM employees;

-- NULL in CASE statements
SELECT name,
       CASE 
           WHEN bonus IS NULL THEN 'No bonus'
           WHEN bonus > 10000 THEN 'High bonus'
           ELSE 'Standard bonus'
       END as bonus_category
FROM employees;`,
          wrongAnswers: [
            "NULL = NULL returns TRUE in SQL.",
            "COUNT(*) and COUNT(column) always return the same result.",
            "AVG() treats NULL values as zero."
          ]
        },
        {
          question: "Explain different types of JOINs with examples of when to use each",
          answer: "INNER JOIN returns matching rows, LEFT JOIN includes all from left table, RIGHT JOIN all from right, FULL OUTER all from both, CROSS JOIN cartesian product.",
          code: `-- INNER JOIN: Only matching records
SELECT e.name, d.dept_name
FROM employees e
INNER JOIN departments d ON e.dept_id = d.id;

-- LEFT JOIN: All employees, even without department
SELECT e.name, d.dept_name
FROM employees e
LEFT JOIN departments d ON e.dept_id = d.id;

-- RIGHT JOIN: All departments, even without employees
SELECT e.name, d.dept_name
FROM employees e
RIGHT JOIN departments d ON e.dept_id = d.id;

-- FULL OUTER JOIN: All employees and all departments
SELECT e.name, d.dept_name
FROM employees e
FULL OUTER JOIN departments d ON e.dept_id = d.id;

-- CROSS JOIN: Cartesian product (rarely used)
SELECT e.name, p.project_name
FROM employees e
CROSS JOIN projects p;

-- Self JOIN: Compare within same table
SELECT e1.name as employee, e2.name as manager
FROM employees e1
LEFT JOIN employees e2 ON e1.manager_id = e2.id;`,
          wrongAnswers: [
            "LEFT JOIN and RIGHT JOIN always return the same results.",
            "INNER JOIN includes NULL values from both tables.",
            "CROSS JOIN is the most commonly used join type."
          ]
        }
      ],
      "Statistics & Probability": [
        {
          question: "Explain the Central Limit Theorem",
          answer: "The CLT states that the sampling distribution of sample means approaches a normal distribution as sample size increases, regardless of the population's distribution.",
          code: `import numpy as np
import matplotlib.pyplot as plt

# Simulate CLT
population = np.random.exponential(2, 10000)  # Non-normal population
sample_means = []

for _ in range(1000):
    sample = np.random.choice(population, 30)
    sample_means.append(np.mean(sample))

# sample_means will be approximately normal`,
          wrongAnswers: [
            "The CLT states that all data becomes normally distributed with large samples.",
            "The CLT only applies to populations that are already normally distributed.",
            "The CLT requires a minimum sample size of 1000 to be valid."
          ]
        },
        {
          question: "What's the difference between correlation and causation?",
          answer: "Correlation measures statistical relationship between variables. Causation means one variable directly influences another. Correlation doesn't imply causation.",
          code: `# Example: Ice cream sales and drowning deaths are correlated
# But ice cream doesn't cause drowning - temperature is the confounding variable`,
          wrongAnswers: [
            "Correlation always implies causation if the correlation is strong enough.",
            "Causation is just another word for negative correlation.",
            "There is no difference; these terms are interchangeable in statistics."
          ]
        },
        {
          question: "Explain Type I and Type II errors in hypothesis testing",
          answer: "Type I error (false positive): Rejecting a true null hypothesis. Type II error (false negative): Failing to reject a false null hypothesis. Œ± controls Type I, Œ≤ controls Type II.",
          code: `# Type I Error (Œ±): False Positive
# H0: Drug has no effect (true)
# Result: We conclude drug works (reject H0)
# Consequence: Ineffective drug goes to market

# Type II Error (Œ≤): False Negative  
# H0: Drug has no effect (false - drug actually works)
# Result: We conclude drug doesn't work (fail to reject H0)
# Consequence: Effective drug is discarded

# Power = 1 - Œ≤ (probability of detecting true effect)
# Sample size calculation example:
from statsmodels.stats.power import TTestPower
analysis = TTestPower()
sample_size = analysis.solve_power(effect_size=0.5, power=0.8, alpha=0.05)`,
          wrongAnswers: [
            "Type I error is failing to reject a false null hypothesis.",
            "Type II error is more serious than Type I error in all cases.",
            "Increasing sample size increases both Type I and Type II errors."
          ]
        },
        {
          question: "What's the difference between standard deviation and standard error?",
          answer: "Standard deviation measures spread of data points around the mean. Standard error measures precision of sample mean as estimate of population mean. SE = SD/‚àön.",
          code: `import numpy as np

# Population
population = np.random.normal(100, 15, 10000)  # Œº=100, œÉ=15

# Single sample
sample = np.random.choice(population, 100)
sd = np.std(sample, ddof=1)  # Standard deviation of sample
se = sd / np.sqrt(len(sample))  # Standard error

print(f"Sample SD: {sd:.2f}")  # Measures spread in sample
print(f"Sample SE: {se:.2f}")  # Measures uncertainty of mean

# Multiple samples to demonstrate
sample_means = [np.mean(np.random.choice(population, 100)) for _ in range(1000)]
print(f"SD of sample means: {np.std(sample_means):.2f}")  # Should ‚âà SE`,
          wrongAnswers: [
            "Standard error is just another name for standard deviation.",
            "Standard error increases as sample size increases.",
            "Standard deviation of the sampling distribution equals population standard deviation."
          ]
        },
        {
          question: "Explain p-value and its interpretation in hypothesis testing",
          answer: "P-value is the probability of observing test results at least as extreme as actual results, assuming null hypothesis is true. Small p-value suggests data is inconsistent with H0.",
          code: `from scipy import stats

# Example: Testing if mean differs from expected
sample = [98, 102, 97, 105, 103, 99, 101, 100, 98, 104]
expected_mean = 100

# One-sample t-test
t_stat, p_value = stats.ttest_1samp(sample, expected_mean)

print(f"p-value: {p_value:.4f}")

# Interpretation:
# If p < 0.05: "Statistically significant" - reject H0
# If p ‚â• 0.05: "Not statistically significant" - fail to reject H0

# IMPORTANT: p-value is NOT:
# - Probability that H0 is true
# - Probability that results occurred by chance
# - Measure of effect size or practical importance`,
          wrongAnswers: [
            "P-value is the probability that the null hypothesis is true.",
            "A p-value of 0.05 means there's a 5% chance the results are due to chance.",
            "Lower p-values always indicate more important or meaningful results."
          ]
        }
      ],
      "Data Cleaning & Preprocessing": [
        {
          question: "How do you detect and handle outliers?",
          answer: "Multiple methods: IQR method, Z-score method, or visualization with box plots.",
          code: `import pandas as pd
import numpy as np

data = pd.Series([1, 2, 3, 4, 5, 100])  # 100 is an outlier

# Method 1: IQR method
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = data[(data < lower_bound) | (data > upper_bound)]

# Method 2: Z-score method
z_scores = np.abs((data - data.mean()) / data.std())
outliers = data[z_scores > 3]

# Method 3: Visualization
data.boxplot()  # Box plots show outliers`,
          wrongAnswers: [
            "Outliers should always be removed from the dataset without investigation.",
            "The only way to detect outliers is by manually inspecting each data point.",
            "Outliers are defined as any value above the mean."
          ]
        },
        {
          question: "How do you handle categorical variables?",
          answer: "Multiple encoding techniques: Label Encoding for ordinal data, One-Hot Encoding for nominal data, or Target encoding for supervised learning.",
          code: `import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

df = pd.DataFrame({'color': ['red', 'blue', 'green', 'red']})

# Method 1: Label Encoding (ordinal)
le = LabelEncoder()
df['color_encoded'] = le.fit_transform(df['color'])

# Method 2: One-Hot Encoding (nominal)
df_encoded = pd.get_dummies(df, columns=['color'])

# Method 3: Target encoding (for supervised learning)
# Encode based on target variable relationship`,
          wrongAnswers: [
            "Categorical variables must be removed before analysis.",
            "All categorical variables should be converted to the string 'category'.",
            "Categorical variables can be used directly in all machine learning models."
          ]
        },
        {
          question: "Explain different methods for handling imbalanced datasets",
          answer: "Use resampling techniques (oversampling minority/undersampling majority), SMOTE, class weights, or ensemble methods. Choose based on data size and problem context.",
          code: `from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.utils import resample

# Original imbalanced data
df_majority = df[df.target==0]  # 900 samples
df_minority = df[df.target==1]  # 100 samples

# 1. Oversampling (duplicate minority)
df_minority_upsampled = resample(df_minority, 
                                replace=True,
                                n_samples=900)
df_balanced = pd.concat([df_majority, df_minority_upsampled])

# 2. Undersampling (reduce majority)
df_majority_downsampled = resample(df_majority,
                                  replace=False,
                                  n_samples=100)

# 3. SMOTE (synthetic examples)
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

# 4. Class weights (no resampling)
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(class_weight='balanced')`,
          wrongAnswers: [
            "Imbalanced datasets don't affect model performance.",
            "The only solution is to collect more data for the minority class.",
            "Always use oversampling as it's the most effective method."
          ]
        },
        {
          question: "What is feature scaling and when should you use normalization vs standardization?",
          answer: "Feature scaling ensures features have similar ranges. Use normalization (Min-Max) for bounded data and known range. Use standardization (Z-score) for normally distributed data or unknown bounds.",
          code: `from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

# Normalization (Min-Max Scaling) - scales to [0,1]
# Use when: bounded data, know min/max won't change
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# Standardization (Z-score) - mean=0, std=1
# Use when: Gaussian distribution, unbounded, outliers under control
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Robust Scaling - uses median and IQR
# Use when: outliers present
scaler = RobustScaler()
X_robust = scaler.fit_transform(X)

# When NOT to scale:
# - Tree-based models (Random Forest, XGBoost)
# - Already meaningful scales (age, money)
# When TO scale:
# - Distance-based algorithms (KNN, K-means, SVM)
# - Neural networks
# - PCA`,
          wrongAnswers: [
            "Feature scaling is only needed for neural networks.",
            "Normalization and standardization produce identical results.",
            "Tree-based models require feature scaling to work properly."
          ]
        },
        {
          question: "How do you handle data leakage and why is it dangerous?",
          answer: "Data leakage occurs when training data contains information about the target that won't be available during prediction. Prevent by careful feature engineering, proper train/test splitting, and temporal awareness.",
          code: `# Common data leakage examples and solutions:

# 1. Target leakage - feature contains target information
# BAD: Using "loan_approved" to predict "will_default"
# GOOD: Only use features available at prediction time

# 2. Train-test contamination
# BAD: Scaling before splitting
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Don't do this!
X_train, X_test = train_test_split(X_scaled)

# GOOD: Scale after splitting
X_train, X_test = train_test_split(X)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Only transform!

# 3. Temporal leakage
# BAD: Random split on time series
# GOOD: Time-based split
train_data = df[df['date'] < '2023-01-01']
test_data = df[df['date'] >= '2023-01-01']

# 4. Duplicate data
# Remove duplicates that might be in both train/test
df = df.drop_duplicates()`,
          wrongAnswers: [
            "Data leakage improves model performance and should be encouraged.",
            "Data leakage only occurs when you accidentally include the target variable as a feature.",
            "Scaling data before train-test split doesn't cause leakage."
          ]
        }
      ],
      "Machine Learning Basics": [
        {
          question: "Explain overfitting and how to prevent it",
          answer: "Overfitting occurs when a model learns training data too specifically and fails to generalize. Prevention: train/validation/test split, cross-validation, regularization, early stopping, dropout, reduce model complexity, or get more data.",
          code: `from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor

# 1. Train/validation/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 2. Cross-validation
scores = cross_val_score(model, X_train, y_train, cv=5)

# 3. Regularization (for linear models)
from sklearn.linear_model import Ridge, Lasso
ridge = Ridge(alpha=1.0)  # L2 regularization
lasso = Lasso(alpha=1.0)  # L1 regularization

# 4. Early stopping, dropout (for neural networks)
# 5. Reduce model complexity
# 6. Get more training data`,
          wrongAnswers: [
            "Overfitting means the model is too simple and needs more parameters.",
            "Overfitting can be prevented by training the model for more epochs.",
            "Overfitting only occurs in linear models, not in tree-based models."
          ]
        },
        {
          question: "What's the bias-variance tradeoff?",
          answer: "Bias is error from oversimplifying assumptions. Variance is error from sensitivity to small fluctuations. High bias = underfitting, high variance = overfitting. Goal is to minimize total error = bias¬≤ + variance + noise.",
          code: `# High bias model: Linear regression on non-linear data
# High variance model: Deep decision tree on small dataset
# Balanced model: Random forest with proper hyperparameters`,
          wrongAnswers: [
            "Bias and variance are the same thing with different names.",
            "High bias always leads to better model performance.",
            "The goal is to maximize both bias and variance."
          ]
        },
        {
          question: "Explain different validation strategies and when to use each",
          answer: "Train-test split for large datasets, k-fold cross-validation for medium datasets, stratified k-fold for imbalanced data, time series split for temporal data, and leave-one-out for very small datasets.",
          code: `from sklearn.model_selection import (train_test_split, KFold, 
                                     StratifiedKFold, TimeSeriesSplit, LeaveOneOut)

# 1. Simple train-test split (fast, large datasets)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. K-Fold Cross-Validation (better estimate, medium datasets)
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kfold)

# 3. Stratified K-Fold (imbalanced datasets)
skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skfold)

# 4. Time Series Split (temporal data)
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    # Train and evaluate

# 5. Leave-One-Out (very small datasets <100 samples)
loo = LeaveOneOut()
scores = cross_val_score(model, X, y, cv=loo)`,
          wrongAnswers: [
            "Always use train-test split regardless of dataset size or type.",
            "K-fold cross-validation should be used for time series data.",
            "Stratified k-fold is only for regression problems."
          ]
        },
        {
          question: "What evaluation metrics would you use for classification vs regression problems?",
          answer: "Classification: accuracy, precision, recall, F1-score, ROC-AUC. Regression: MAE, MSE, RMSE, R¬≤. Choose based on business context and data characteristics.",
          code: `# Classification metrics
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix)

# When to use each:
# - Accuracy: Only when classes are balanced
# - Precision: When false positives are costly (spam detection)
# - Recall: When false negatives are costly (disease detection)
# - F1: Balance between precision and recall
# - ROC-AUC: Overall performance across thresholds

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# For probability predictions
y_pred_proba = model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_pred_proba)

# Regression metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# When to use each:
# - MAE: Robust to outliers, easy to interpret
# - MSE/RMSE: Penalizes large errors more
# - R¬≤: Percentage of variance explained

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)`,
          wrongAnswers: [
            "Accuracy is always the best metric for classification problems.",
            "MSE and MAE always give the same model rankings.",
            "R¬≤ can be used for classification problems."
          ]
        },
        {
          question: "Explain the difference between parametric and non-parametric models with examples",
          answer: "Parametric models assume fixed form with finite parameters (linear regression, logistic regression). Non-parametric models don't assume functional form and can grow with data (KNN, decision trees, SVM).",
          code: `# Parametric Models - fixed number of parameters
# Linear Regression: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œµ
from sklearn.linear_model import LinearRegression
lr = LinearRegression()  # Always has n_features + 1 parameters

# Logistic Regression
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()

# Advantages: Fast, less data needed, interpretable
# Disadvantages: Limited flexibility, strong assumptions

# Non-parametric Models - parameters grow with data
# K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)  # Stores all training data

# Decision Trees
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier()  # Tree size depends on data

# Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()  # Multiple trees, complexity varies

# Advantages: Flexible, few assumptions, can model complex patterns
# Disadvantages: Need more data, prone to overfitting, less interpretable`,
          wrongAnswers: [
            "Non-parametric models have no parameters at all.",
            "Decision trees are parametric because they have hyperparameters.",
            "Parametric models always perform worse than non-parametric models."
          ]
        }
      ],
      "Performance & Optimization": [
        {
          question: "How do you optimize pandas operations?",
          answer: "Use vectorized operations instead of loops, .loc for label-based indexing, categorical data types, .query() for complex filtering, .eval() for arithmetic operations, and read only needed columns.",
          code: `import pandas as pd
import numpy as np

# 1. Use vectorized operations instead of loops
df['new_col'] = df['col1'] * df['col2']  # Good
# Don't: df['new_col'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)

# 2. Use .loc for label-based indexing
df.loc[df['column'] > 5, 'new_col'] = 'high'

# 3. Use categorical data types
df['category'] = df['category'].astype('category')

# 4. Use .query() for complex filtering
df.query('column1 > 5 and column2 < 10')

# 5. Use .eval() for arithmetic operations
df.eval('new_col = col1 + col2 * col3')

# 6. Read only needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2'])`,
          wrongAnswers: [
            "Always use apply() for better performance with pandas operations.",
            "Converting to lists and using Python loops is faster than pandas operations.",
            "Reading all columns and then dropping is more efficient than selecting columns."
          ]
        },
        {
          question: "When would you use apply() vs map() vs applymap()?",
          answer: "apply(): Apply function along axis (rows/columns). map(): Element-wise transformation for Series only. applymap(): Element-wise transformation for entire DataFrame.",
          code: `df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})

# apply() - along axis
df.apply(sum)  # Sum each column
df.apply(lambda x: x.max() - x.min(), axis=1)  # Range of each row

# map() - Series only
df['A'].map(lambda x: x**2)

# applymap() - element-wise on DataFrame
df.applymap(lambda x: x**2)`,
          wrongAnswers: [
            "All three methods do the same thing and are interchangeable.",
            "map() works on DataFrames, applymap() works on Series.",
            "apply() can only be used with built-in functions, not lambda functions."
          ]
        },
        {
          question: "How do you profile and optimize Python code performance?",
          answer: "Use profiling tools (cProfile, line_profiler), identify bottlenecks, then optimize using vectorization, caching, better algorithms, or Cython/Numba for critical sections.",
          code: `# 1. Basic timing
import time
start = time.time()
# Your code here
print(f"Execution time: {time.time() - start:.4f} seconds")

# 2. Using cProfile
import cProfile
cProfile.run('your_function()')

# 3. Line profiler (install: pip install line_profiler)
# @profile decorator on function, then run: kernprof -lv script.py

# 4. Memory profiling
import sys
sys.getsizeof(object)  # Memory usage of object

# 5. Optimization techniques:
# a) Use NumPy for numerical operations
list_sum = sum([i**2 for i in range(1000000)])  # Slow
array_sum = np.sum(np.arange(1000000)**2)  # Fast

# b) Cache expensive computations
from functools import lru_cache
@lru_cache(maxsize=128)
def expensive_function(n):
    return n ** 2

# c) Use Numba for JIT compilation
from numba import jit
@jit
def fast_function(arr):
    return np.sum(arr ** 2)`,
          wrongAnswers: [
            "Python code cannot be optimized; you must rewrite in C++ for performance.",
            "Adding more print statements makes code run faster.",
            "List comprehensions are always slower than for loops."
          ]
        },
        {
          question: "Explain chunking strategy for processing large datasets that don't fit in memory",
          answer: "Process data in smaller chunks using pandas chunksize parameter, Dask for parallel processing, or generators for streaming. Aggregate results incrementally.",
          code: `# 1. Pandas chunking
chunk_size = 10000
running_sum = 0
running_count = 0

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    chunk_sum = chunk['value'].sum()
    running_sum += chunk_sum
    running_count += len(chunk)

average = running_sum / running_count

# 2. Using Dask for parallel processing
import dask.dataframe as dd
ddf = dd.read_csv('large_file.csv')
result = ddf.groupby('category').value.mean().compute()

# 3. Generator for memory-efficient processing
def read_large_file(file_path):
    with open(file_path, 'r') as file:
        for line in file:
            yield process_line(line)

# 4. SQL-like operations on chunks
def process_in_chunks(filename, agg_func):
    result = {}
    for chunk in pd.read_csv(filename, chunksize=10000):
        grouped = chunk.groupby('key').agg(agg_func)
        for key, value in grouped.iterrows():
            if key not in result:
                result[key] = []
            result[key].append(value)
    
    # Final aggregation
    return {k: agg_func(v) for k, v in result.items()}`,
          wrongAnswers: [
            "Always load entire datasets into memory for best performance.",
            "Chunking makes processing slower and should be avoided.",
            "You can only process files that fit in available RAM."
          ]
        },
        {
          question: "What are best practices for handling time series data efficiently?",
          answer: "Use proper datetime indexing, resample for aggregations, rolling windows for moving statistics, and shift/diff for lag features. Store in appropriate formats like Parquet for better performance.",
          code: `# 1. Proper datetime indexing
df['date'] = pd.to_datetime(df['date'])
df = df.set_index('date')
df = df.sort_index()  # Ensure chronological order

# 2. Resampling for different frequencies
daily_data = df.resample('D').sum()  # Daily aggregation
monthly_avg = df.resample('M').mean()  # Monthly average

# 3. Rolling window operations
df['ma_7'] = df['value'].rolling(window=7).mean()
df['rolling_std'] = df['value'].rolling(window=30).std()

# 4. Lag features and differences
df['lag_1'] = df['value'].shift(1)
df['lag_7'] = df['value'].shift(7)
df['diff_1'] = df['value'].diff(1)  # First difference
df['pct_change'] = df['value'].pct_change()

# 5. Efficient storage
# Parquet format for time series
df.to_parquet('timeseries.parquet', engine='pyarrow')
df_loaded = pd.read_parquet('timeseries.parquet')

# 6. Vectorized date operations
df['day_of_week'] = df.index.dayofweek
df['month'] = df.index.month
df['is_weekend'] = df.index.weekday >= 5`,
          wrongAnswers: [
            "Store timestamps as strings for better performance.",
            "Always use loops to calculate rolling statistics.",
            "CSV is the most efficient format for time series data."
          ]
        }
      ]
    };

    // Test mode variables
    let testQuestions = [];
    let currentQuestionIndex = 0;
    let userAnswers = [];
    let correctAnswers = 0;
    let currentTestType = 'all';

    // Icons for each category
    const categoryIcons = {
      "Python Fundamentals": "üêç",
      "Pandas Questions": "üêº",
      "NumPy Questions": "üî¢",
      "SQL Questions": "üíæ",
      "Statistics & Probability": "üìä",
      "Data Cleaning & Preprocessing": "üßπ",
      "Machine Learning Basics": "ü§ñ",
      "Performance & Optimization": "‚ö°"
    };

    // Initialize the page
    function init() {
      generateTopicNavigation();
      generateStudyContent();
      generateTestOptions();
    }

    // Generate topic navigation
    function generateTopicNavigation() {
      const topicNavGrid = document.getElementById('topicNavGrid');
      let html = '';

      for (const [category, questions] of Object.entries(questionData)) {
        const categoryId = category.replace(/\s+/g, '-').toLowerCase();
        html += `
          <button class="topic-nav-btn" onclick="scrollToTopic('${categoryId}')">
            <span class="topic-nav-icon">${categoryIcons[category]}</span>
            <span>${category}</span>
          </button>
        `;
      }

      topicNavGrid.innerHTML = html;
    }

    // Scroll to topic
    function scrollToTopic(categoryId) {
      const element = document.getElementById(categoryId);
      if (element) {
        const offset = 80; // Offset for fixed header
        const elementPosition = element.getBoundingClientRect().top;
        const offsetPosition = elementPosition + window.pageYOffset - offset;

        window.scrollTo({
          top: offsetPosition,
          behavior: 'smooth'
        });
      }
    }

    // Generate study content
    function generateStudyContent() {
      const studyQuestions = document.getElementById('studyQuestions');
      let html = '';

      for (const [category, questions] of Object.entries(questionData)) {
        const categoryId = category.replace(/\s+/g, '-').toLowerCase();
        html += `<div class="category-section" id="${categoryId}">
          <h2 class="category-title">${categoryIcons[category]} ${category}</h2>`;
        
        questions.forEach((q, index) => {
          html += `
            <div class="question-card">
              <h3 class="question">Q${index + 1}: ${q.question}</h3>
              <p class="answer">${q.answer}</p>
              ${q.code ? `<div class="code-block"><pre>${escapeHtml(q.code)}</pre></div>` : ''}
            </div>
          `;
        });
        
        html += '</div>';
      }

      studyQuestions.innerHTML = html;
    }

    // Generate test options
    function generateTestOptions() {
      const testOptionsGrid = document.getElementById('testOptionsGrid');
      let html = '';

      // All topics option
      html += `
        <div class="test-option-card" onclick="startTest('all')">
          <div class="test-option-icon">üéØ</div>
          <div class="test-option-title">All Topics</div>
          <div class="test-option-count">20 questions from all categories</div>
        </div>
      `;

      // Individual category options
      for (const [category, questions] of Object.entries(questionData)) {
        html += `
          <div class="test-option-card" onclick="startTest('${category}')">
            <div class="test-option-icon">${categoryIcons[category]}</div>
            <div class="test-option-title">${category}</div>
            <div class="test-option-count">${questions.length} questions</div>
          </div>
        `;
      }

      testOptionsGrid.innerHTML = html;
    }

    // Switch between study and test modes
    function switchMode(mode) {
      const studyContent = document.getElementById('studyContent');
      const testSelector = document.getElementById('testSelector');
      const testContent = document.getElementById('testContent');
      const testResults = document.getElementById('testResults');
      const modeBtns = document.querySelectorAll('.mode-btn');

      modeBtns.forEach(btn => btn.classList.remove('active'));

      if (mode === 'study') {
        studyContent.classList.add('active');
        testSelector.style.display = 'none';
        testContent.classList.remove('active');
        testResults.classList.remove('active');
        modeBtns[0].classList.add('active');
      } else {
        studyContent.classList.remove('active');
        testSelector.style.display = 'block';
        testContent.classList.remove('active');
        testResults.classList.remove('active');
        modeBtns[1].classList.add('active');
      }
    }

    // Start test
    function startTest(testType) {
      currentTestType = testType;
      
      // Reset test variables
      testQuestions = [];
      currentQuestionIndex = 0;
      userAnswers = [];
      correctAnswers = 0;

      // Collect questions based on test type
      if (testType === 'all') {
        for (const questions of Object.values(questionData)) {
          testQuestions.push(...questions);
        }
        // Shuffle and take 20 questions for comprehensive test
        testQuestions = shuffleArray(testQuestions).slice(0, 20);
        document.getElementById('testTitle').textContent = 'All Topics Test';
        document.getElementById('testSubtitle').textContent = '20 questions from all categories';
      } else {
        testQuestions = [...questionData[testType]];
        testQuestions = shuffleArray(testQuestions);
        document.getElementById('testTitle').textContent = `${testType} Test`;
        document.getElementById('testSubtitle').textContent = `${testQuestions.length} questions`;
      }

      // Hide test selector and show test content
      document.getElementById('testSelector').style.display = 'none';
      document.getElementById('testResults').classList.remove('active');
      document.getElementById('testContent').classList.add('active');
      
      // Display first question
      displayQuestion();
    }

    // Display current question
    function displayQuestion() {
      const question = testQuestions[currentQuestionIndex];
      const testQuestionCard = document.getElementById('testQuestionCard');
      
      // Create options (1 correct + 3 wrong)
      const options = [
        { text: question.answer, isCorrect: true },
        ...question.wrongAnswers.map(answer => ({ text: answer, isCorrect: false }))
      ];
      
      // Shuffle options
      const shuffledOptions = shuffleArray(options);

      let html = `
        <h2 class="test-question">${question.question}</h2>
        <div class="test-options">
      `;

      shuffledOptions.forEach((option, index) => {
        html += `
          <div class="test-option" onclick="selectAnswer(${index}, ${option.isCorrect})" data-index="${index}">
            ${option.text}
          </div>
        `;
      });

      html += '</div>';
      testQuestionCard.innerHTML = html;

      // Update progress
      updateProgress();

      // Update navigation buttons
      document.getElementById('prevBtn').disabled = currentQuestionIndex === 0;
      document.getElementById('nextBtn').textContent = 
        currentQuestionIndex === testQuestions.length - 1 ? 'Finish' : 'Next';
    }

    // Handle answer selection
    function selectAnswer(optionIndex, isCorrect) {
      // Disable all options
      const options = document.querySelectorAll('.test-option');
      options.forEach(option => {
        option.classList.add('disabled');
      });

      // Mark selected option
      const selectedOption = document.querySelector(`[data-index="${optionIndex}"]`);
      if (isCorrect) {
        selectedOption.classList.add('correct');
        if (!userAnswers[currentQuestionIndex]) {
          correctAnswers++;
        }
      } else {
        selectedOption.classList.add('incorrect');
        // Show correct answer
        options.forEach(option => {
          if (option.textContent.trim() === testQuestions[currentQuestionIndex].answer) {
            option.classList.add('correct');
          }
        });
      }

      // Record answer
      userAnswers[currentQuestionIndex] = isCorrect;
    }

    // Navigate to next question
    function nextQuestion() {
      if (currentQuestionIndex < testQuestions.length - 1) {
        currentQuestionIndex++;
        displayQuestion();
      } else {
        showResults();
      }
    }

    // Navigate to previous question
    function previousQuestion() {
      if (currentQuestionIndex > 0) {
        currentQuestionIndex--;
        displayQuestion();
      }
    }

    // Update progress bar
    function updateProgress() {
      const progress = ((currentQuestionIndex + 1) / testQuestions.length) * 100;
      document.getElementById('progressBar').style.width = `${progress}%`;
    }

    // Show test results
    function showResults() {
      const testContent = document.getElementById('testContent');
      const testResults = document.getElementById('testResults');
      
      testContent.classList.remove('active');
      testResults.classList.add('active');

      const percentage = Math.round((correctAnswers / testQuestions.length) * 100);
      
      document.getElementById('resultsScore').textContent = `${correctAnswers}/${testQuestions.length}`;
      
      let message;
      if (percentage >= 90) {
        message = "Outstanding! You're ready for any data analyst interview! üåü";
      } else if (percentage >= 80) {
        message = "Great job! You have a strong grasp of the concepts! üéØ";
      } else if (percentage >= 70) {
        message = "Good work! Review a few more topics and you'll be interview-ready! üìö";
      } else if (percentage >= 60) {
        message = "Not bad! Keep studying and you'll improve quickly! üí™";
      } else {
        message = "Keep practicing! Every expert was once a beginner! üöÄ";
      }
      
      document.getElementById('resultsMessage').textContent = message;
    }

    // Retake current test
    function retakeCurrentTest() {
      startTest(currentTestType);
    }

    // Utility functions
    function shuffleArray(array) {
      const newArray = [...array];
      for (let i = newArray.length - 1; i > 0; i--) {
        const j = Math.floor(Math.random() * (i + 1));
        [newArray[i], newArray[j]] = [newArray[j], newArray[i]];
      }
      return newArray;
    }

    function escapeHtml(text) {
      const map = {
        '&': '&amp;',
        '<': '&lt;',
        '>': '&gt;',
        '"': '&quot;',
        "'": '&#039;'
      };
      return text.replace(/[&<>"']/g, m => map[m]);
    }

    // Initialize on load
    init();
  </script>
</body>
</html>